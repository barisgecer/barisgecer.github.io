<!--
Author: W3layouts
Author URL: http://w3layouts.com
License: Creative Commons Attribution 3.0 Unported
License URL: http://creativecommons.org/licenses/by/3.0/
-->
<html><head>
<title>Baris Gecer</title>
<link href="css/bootstrap.css" rel="stylesheet" type="text/css">
<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
<script src="js/jquery.min.js"></script>
<!-- Custom Theme files -->
<link href="css/style.css" rel="stylesheet" type="text/css">

<link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

 <!-- Custom Theme files -->
 <meta name="viewport" content="width=device-width, initial-scale=1">
 <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
 <meta name="keywords" content="Baris Gecer academic webpage, Computer Vision, Deep Learning, Medical Image Analysis, Imperial College London, facegan, Color COSFIRE, generative adversarial networks">
<!-- webfonts -->
	<link href="//fonts.googleapis.com/css?family=Asap:400,700,400italic" rel="stylesheet" type="text/css">
	<link href="//fonts.googleapis.com/css?family=Open+Sans:400,300,600" rel="stylesheet" type="text/css">
<!-- webfonts -->
 <!---- start-smoth-scrolling---->
<script type="text/javascript" src="js/move-top.js"></script>
<script type="text/javascript" src="js/easing.js"></script>
	<script type="text/javascript">
			jQuery(document).ready(function($) {
				$(".scroll").click(function(event){
					event.preventDefault();
					$('html,body').animate({scrollTop:$(this.hash).offset().top},1000);
				});
			});
		</script>
 <!---- start-smoth-scrolling---->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-18620166-1', 'auto');
  ga('send', 'pageview');

</script>

</head>
	<body class="" style="">
		<!-- container -->
			<!-- header -->
			<div id="home" class="header">
				<div class="container">
				<!-- top-hedader -->
				<div class="top-header">
					<!-- /logo -->
					<!--top-nav---->
					<div class="top-nav">
					<div class="navigation">
					<div class="logo">
						<h1><img src="images/logo.png" alt="" style="
    height: 50;
"></h1>
					</div>
					<div class="navigation-right">
						<span class="menu"><img src="images/menu.png" alt=" "></span>
						<nav class="link-effect-3" id="link-effect-3">
							<ul class="nav1 nav nav-wil">
								<li class="active"><a data-hover="Home" href="index.html">Home</a></li>
								<li><a class="scroll" data-hover="About" href="#about_div">About</a></li>

								<li><a class="scroll" data-hover="News" href="#work">News</a></li>

								<li><a class="scroll" data-hover="Publications" href="#blogs">Publications</a></li>

							</ul>
						</nav>
							<!-- script-for-menu -->
								<script>
								   $( "span.menu" ).click(function() {
									 $( "ul.nav1" ).slideToggle( 300, function() {
									 // Animation complete.
									  });
									 });
								</script>
							<!-- /script-for-menu -->
					</div>
					<div class="clearfix"></div>
				</div>
				<!-- /top-hedader -->
				</div>
			<div class="banner-info" style="
    background: rgba(23, 20, 20, 0.9);
">
				<div id="about_div" class="col-md-7 header-right">
					<h1>Barış Geçer</h1>
					<h6 style="letter-spacing: 1.5px;">Ph.D. student @ iBUG, Imperial College London</h6>
					<h6 style="letter-spacing: 1.5px; font-size: 12px; text-transform: capitalize;" >Subject: 3D Computer Vision and Deep Learning</h6>
					<ul class="address">
						<li>
							<ul class="address-text">

								<li style="
    width: 100%;
">I am a Ph.D. student at Imperial College London, supervised by <a href="https://wp.doc.ic.ac.uk/szafeiri/">Dr. Stefanos Zafeiriou</a> from <a href="https://ibug.doc.ic.ac.uk/">iBUG</a> and Computer Vision and Deep Learning Scientist at <a href="https://facesoft.io/">Facesoft</a>. Recently, I was a Research Intern at <a href="https://www.facebook.com/careers/areas-of-work/facebookrealitylabs/?teams[0]=Facebook%20Reality%20Labs">Facebook Reality Labs</a>, supervised by <a href="http://www.cs.cmu.edu/~ftorre/">Dr. Fernando De La Torre</a>, and worked on <a href="https://tech.fb.com/inside-facebook-reality-labs-research-updates-and-the-future-of-social-connection/">Codec Avatars</a>.
<br><br>
My research goal is to contribute to projecting Human Facial Biometrics into the <a href="https://en.wikipedia.org/wiki/Mirror_world">Mirror Worlds</a>. Meaning that I study high-quality digitalization of Faces into 3D (i.e. AR/VR) while identity-, expression- and motion-related attributes are preserved. For that, I am currently exploring photorealistic 3D Face reconstruction, modeling, and synthesis by Generative Adversarial Nets and Deep Learning.

<br><br>

 <!----I obtained my M.S. degree from <a href="http://www.cs.bilkent.edu.tr/"> Bilkent University Computer Engineering department</a> under the supervision of <a href="http://www.cs.bilkent.edu.tr/~saksoy/">Prof. Selim Aksoy</a> in 2016. And I obtained my undergraduate degree in Computer Engineering from Hacettepe University in 2014. I also did an internship in <a href="http://www.cs.rug.nl/is/">Intelligent Systems Lab of the University of Groningen</a>., with the supervision of <a href="http://www.cs.rug.nl/~petkov/">Prof. Nicolai Petkov</a> and <a href="http://www.cs.rug.nl/~george/">Dr. George Azzopardi</a>.-->
</li>
							</ul>
						</li><li>
							<ul class="address-text">
								<li><b>E-mail</b></li>
								<li><a href="http://www.google.com/recaptcha/mailhide/d?k=01bIviUEI_nyIBg4Vb679KBg==&amp;c=TmoTkCRzDgb3MB66XLBeTOObdgIj5-hzHcTIdXHJd3E=" onclick="window.open('http://www.google.com/recaptcha/mailhide/d?k\x3d01bIviUEI_nyIBg4Vb679KBg\x3d\x3d\x26c\x3dTmoTkCRzDgb3MB66XLBeTOObdgIj5-hzHcTIdXHJd3E\x3d', '', 'toolbar=0,scrollbars=0,location=0,statusbar=0,menubar=0,resizable=0,width=500,height=300'); return false;" title="Reveal this e-mail address">b...</a>@imperial.ac.uk</li>
							</ul>
						</li><li>
							<ul class="address-text">
								<li><b>Social</b></li>
								<li><p class="glyphicon glyphicon-book"></p><a target="_blank" href="https://scholar.google.com/citations?user=-9X8yaYAAAAJ"> Google Scholar</a>
/<p class="glyphicon glyphicon-indent-left"></p>
<a href="https://github.com/barisgecer" target="_blank">Github</a>
/<p class="glyphicon glyphicon-link"></p>
<a href="https://www.linkedin.com/in/barisgecer/" target="_blank">LinkedIn</a>
/<p class="glyphicon glyphicon-list-alt"></p>
<a href="./files/cv.pdf" target="_blank">CV</a></li>
							</ul>
						</li><li>
							<ul class="address-text">
								<li><b>Research Interest</b></li>
								<li style="
    width: 70%;
">3D Face Modeling/Reconstruction/Synthesis, Generative Adversarial Networks, Computer Vision, Deep Learning, Face Recognition</li>
							</ul>
						</li>



					</ul>
				</div>
				<div class="col-md-5 header-left">
					<img src="images/baris_gecer_narrow_filtered.jpg" alt="">
				</div>
				<div class="clearfix"> </div>

		      </div>
			</div>
		</div>
	</div>



			<div id="work" class="work" style="
    background: #d8dce4;
">
		<div class="container">
			<div class="service-head text-center">
						<h4>NEWS</h4>
						<h3>Professional <span>Activities</span></h3>
						<span class="border one"></span>
					</div>
					<div class="time-main w3l-agile">

					                        <div class="col-md-6 year-info" style="
    width: 15%;
">

							</div><ul class="col-md-6 timeline" style="/* width: 70%; */">
							
																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>Mar / 2020</b> <a href="https://github.com/steliosploumpis/Universal_Head_3DMM">One paper</a> accepted to <a href="http://cvpr2020.thecvf.com/">TPAMI</a>
																	</div>
																  </div>
																</li>
																
																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>Feb / 2020</b> <a href="https://github.com/lattas/AvatarMe">One paper</a> accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>
																	</div>
																  </div>
																</li>
																
																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>October / 2019</b> Started  at <a href="https://facesoft.io/">Facesoft</a> as part-time Computer Vision and Deep Learning Scientist
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>May / 2019</b> Started research internship at <a href="https://www.facebook.com/careers/areas-of-work/facebookrealitylabs/?teams[0]=Facebook%20Reality%20Labs">Facebook Reality Labs</a>
																	</div>
																  </div>
																</li>


																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>January / 2019</b> <a href="./files/gecer_ganfit_cvpr19.pdf">One paper</a> accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>
																	</div>
																  </div>
																</li>


																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>July / 2018</b> <a href="./files/gecer_facegan_2018.pdf">One paper</a> accepted to <a href="https://eccv2018.org/">ECCV 2018</a>
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>July / 2018</b> <a href="./files/gecer_breast_2018.pdf">One paper</a> accepted to <a href="https://www.sciencedirect.com/journal/pattern-recognition/special-issue/1080ZB3SKW5">Pattern Recognition</a>
																	</div>
																  </div>
																</li>

                                <li>
                                	<div class="timeline-panel">
                                	<div class="timeline-body">
                                		<b>June / 2018</b> I recently moved to <a href="https://ibug.doc.ic.ac.uk/">iBUG group</a> for the rest of my PhD</a>
                                	</div>
                                	 </div>
                                </li>

                                <li>
                                	<div class="timeline-panel">
                                	<div class="timeline-body">
                                		<b>February / 2018</b> Started at <a href="https://www.visioimpulse.com/">Visio Impulse</a> as Part-time Research Engineer</a>
                                	</div>
                                	</div>
                                </li>

                                <li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>September / 2017</b> <a href="./files/gecer_maxmargin_2017.pdf">One paper</a> accepted to one of <a href="http://iccv2017.thecvf.com/">ICCV 2017</a> workshops (<a href="https://web.northeastern.edu/smilelab/AMFG2017/index.html">AMFG 2017</a>)
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>September / 2017</b> Student volunteer at <a href="https://bmvc2017.london/">BMVC 2017</a> as official photographer
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>June / 2017</b> Our team is awarded a $20k worth of <a href="https://azure.microsoft.com/en-us/?v=17.29">Microsoft Azure Research</a> grant
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>January / 2017</b> Code for our Color-blob-based COSFIRE is available on <a href="https://github.com/barisgecer/Color-blob-based-COSFIRE">Github</a>
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>November / 2016</b> Our <a href="./files/gecer_IVC_2017.pdf">Color-blob-based COSFIRE</a> work is published in Elsevier Image and Vision Computing
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>October / 2016</b> Started PhD. program at Imperial College
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>July / 2016</b> Obtained M.S. degree from Computer Engineering at Bilkent University
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	<b>July / 2015</b>  Participated <a href="http://svg.dmi.unict.it/icvss2015/"> International Computer Vision Summer School</a></div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>June / 2014</b> Graduated from Hacettepe University, Computer Engineering Department
																	</div>
																  </div>
																</li>

																<li>
																  <div class="timeline-panel">
																	<div class="timeline-body">
																	  <b>March / 2013</b> Research Intern in Intelligent Systems Lab. at University of Groningen
																	</div>
																  </div>
																</li>
															</ul>
								<div class="clearfix"></div>
								</div>
						</div>
				</div>


    <!-- top-grids -->
				<div class="blog" id="blogs" style=" background: hsl(0, 0%, 91%); ">
					<div class="container">
						<div class="service-head text-center">
						<h4>(See Google Scholar for the Up-to-Date List)</h4>
						<h3><span>Publications</span></h3>
						<span class="border one"></span>
					</div>

  				<div class="news-grid" style=""><div class="col-md-6 news-img" style="height: 170px;">
  					  <a href="#" data-toggle="modal" data-target="#myModal6"> <img src="files/gecer_ganfit_workflow.png" alt=" " class="img-responsive" style=""></a>
  					</div>
  				    <div class="col-md-6 news-text" style="">
  					   <h3> B. Gecer, S. Ploumpis, I. Kotsia, S. Zafeiriou, “GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction” CVPR, 2019.</h3>
  						<ul class="news">
  							<li><i class="glyphicon glyphicon-align-left"></i> <a href="#" data-toggle="modal" data-target="#myModal6">Abstract</a></li><li><i class="glyphicon glyphicon-download-alt"></i> <a href="./files/gecer_ganfit_cvpr19.pdf">pdf</a></li>
  							<li><i class="glyphicon glyphicon-comment"></i> <a href="https://github.com/barisgecer/ganfit">Project Page</a></li>
								<li><i class="glyphicon glyphicon-picture"></i> <a href="./files/gecer_ganfit_poster.pdf">Poster</a></li>

  						<li><i class="glyphicon glyphicon-tags"></i> <a href="./files/gecer_ganfit_cvpr19.txt">BibTeX</a></li>
              <li><i class="glyphicon glyphicon-cloud-download"></i> <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Gecer_GANFIT_Generative_Adversarial_Network_Fitting_for_High_Fidelity_3D_Face_CVPR_2019_paper.html">CVF</a></li></ul>
  					</div>
  					<div class="clearfix" style=""></div>
  				 </div>

					<div class="news-grid" style=""><div class="col-md-6 news-img" style="height: 170px;">
						  <a href="#" data-toggle="modal" data-target="#myModal5"> <img src="files/gecer_facegan_workflow.png" alt=" " class="img-responsive" style=""></a>
						</div>
					    <div class="col-md-6 news-text" style="">
						   <h3> B. Gecer, B. Bhattarai, J. Kittler and TK. Kim, “Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model” ECCV, 2018.</h3>
							<ul class="news">
								<li><i class="glyphicon glyphicon-align-left"></i> <a href="#" data-toggle="modal" data-target="#myModal5">Abstract</a></li><li><i class="glyphicon glyphicon-download-alt"></i> <a href="./files/gecer_facegan_2018.pdf">pdf</a></li>
								<li><i class="glyphicon glyphicon-comment"></i> <a href="https://github.com/barisgecer/facegan">Code</a></li>
                <li><i class="glyphicon glyphicon-picture"></i> <a href="./files/gecer_facegan_poster.pdf">Poster</a></li>

							<li><i class="glyphicon glyphicon-tags"></i> <a href="./files/gecer_facegan_2018.txt">BibTeX</a></li>
              <li><i class="glyphicon glyphicon-cloud-download"></i> <a href="https://doi.org/10.1007/978-3-030-01252-6_14">DOI</a></li>
              <li><i class="glyphicon glyphicon-cloud-download"></i> <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper.html">CVF</a></li></ul>
            </ul>
						</div>
						<div class="clearfix" style=""></div>
					 </div>

					<div class="news-grid" style=""><div class="col-md-6 news-img" style="height: 170px;">
						  <a href="#" data-toggle="modal" data-target="#myModal4"> <img src="files/gecer_breast_workflow.png" alt=" " class="img-responsive" style=""></a>
						</div>
					    <div class="col-md-6 news-text" style="">
						   <h3> B. Gecer, S. Aksoy, E. Mercan L.G. Shapiro, D.L. Weaver and J.G. Elmore, “Detection and classification of cancer in whole slide breast histopathology images using deep convolutional networks” Pattern Recognition, 2018.</h3>
							<ul class="news">
								<li><i class="glyphicon glyphicon-align-left"></i> <a href="#" data-toggle="modal" data-target="#myModal4">Abstract</a></li><li><i class="glyphicon glyphicon-download-alt"></i> <a href="./files/gecer_breast_2018.pdf">pdf</a></li>


							<li><i class="glyphicon glyphicon-tags"></i> <a href="./files/gecer_breast_2018.txt">BibTeX</a></li>
              <li><i class="glyphicon glyphicon-cloud-download"></i> <a href="https://doi.org/10.1016/j.patcog.2018.07.022">DOI</a></li></ul>
						</div>
						<div class="clearfix" style=""></div>
					 </div>

					   <div class="news-grid" style=""><div class="col-md-6 news-img" style="height: 170px;">
						  <a href="#" data-toggle="modal" data-target="#myModal3"> <img src="files/gecer_maxmargin_workflow.png" alt=" " class="img-responsive" style=""></a>
						</div>
					    <div class="col-md-6 news-text" style="">
						   <h3> B. Gecer, V. Balntas, and TK. Kim, “Learning Deep Convolutional Embeddings for Face Representation Using Joint Sample- and Set-based Supervision” ICCVW, 2017</h3>
							<ul class="news">
								<li><i class="glyphicon glyphicon-align-left"></i> <a href="#" data-toggle="modal" data-target="#myModal3">Abstract</a></li>
                <li><i class="glyphicon glyphicon-download-alt"></i> <a href="./files/gecer_maxmargin_2017.pdf">pdf</a></li>
								<li><i class="glyphicon glyphicon-picture"></i> <a href="./files/gecer_maxmargin_poster.pdf">Poster</a></li>


							<li><i class="glyphicon glyphicon-tags"></i> <a href="./files/gecer_maxmargin_2017.txt">BibTeX</a></li>
              <li><i class="glyphicon glyphicon-cloud-download"></i> <a href="
              http://openaccess.thecvf.com/content_ICCV_2017_workshops/w23/html/Gecer_Learning_Deep_Convolutional_ICCV_2017_paper.html">CVF</a></li></ul></ul>
						</div>
						<div class="clearfix" style=""></div>
					 </div>

					 <div class="news-grid" style=""><div class="col-md-6 news-img" style="height: 170px;">
						  <a href="#" data-toggle="modal" data-target="#myModal1"> <img src="files/gecer_IVC_2017_graphical_abstract.png" alt=" " class="img-responsive" style=""></a>
						</div>
					    <div class="col-md-6 news-text" style="">
						   <h3> B. Gecer, G. Azzopardi, and N. Petkov, “Color-blob-based COSFIRE filters for Object Recognition” Image and Vision Computing, vol. 57, pp. 165-174, 2017.</h3>
							<ul class="news">
								<li><i class="glyphicon glyphicon-align-left"></i> <a href="#" data-toggle="modal" data-target="#myModal1">Abstract</a></li><li><i class="glyphicon glyphicon-download-alt"></i> <a href="./files/gecer_IVC_2017.pdf">pdf</a></li>
								<li><i class="glyphicon glyphicon-comment"></i> <a href="https://github.com/barisgecer/Color-blob-based-COSFIRE">Code</a></li>
								<li><i class="glyphicon glyphicon-picture"></i> <a href="./files/gecer_IVC_2017_poster.pdf">Poster</a></li>


							<li><i class="glyphicon glyphicon-tags"></i> <a href="./files/gecer_IVC_2017.txt">BibTeX</a></li>
              <li><i class="glyphicon glyphicon-cloud-download"></i> <a href="https://doi.org/10.1016/j.imavis.2016.10.006">DOI</a></li></ul>
						</div>
						<div class="clearfix" style=""></div>
					 </div>


					<div class="news-grid" style="">
					    		<div class="col-md-6 news-img" style="height: 170px;">
						  <a href="#" data-toggle="modal" data-target="#myModal2"> <img src="files/gecer_thesis_workflow.png" alt=" " class="img-responsive" style=""></a>

						</div>

					    <div class="col-md-6 news-text" style="">
						   <h3> Gecer, Baris. Detection and classification of breast cancer in whole slide histopathology images using deep convolutional networks. Diss. Bilkent University, 2016.</h3>
							<ul class="news">
								<li><i class="glyphicon glyphicon-align-left"></i> <a href="#" data-toggle="modal" data-target="#myModal2">Abstract</a></li>
								<li><i class="glyphicon glyphicon-download-alt"></i> <a href="files/gecer_thesis.pdf">pdf(full)</a></li>
								<li><i class="glyphicon glyphicon-download-alt"></i> <a href="files/gecer_thesis-compressed.pdf">pdf(compressed)</a></li>
								<li><i class="glyphicon glyphicon-tags"></i> <a href="files/gecer_thesis.txt">BibTeX</a></li>
								<li><i class="glyphicon glyphicon-cloud-download"></i> <a href="http://repository.bilkent.edu.tr/handle/11693/32172">REPO</a></li>
							</ul>
						</div>
						<div class="clearfix" style=""></div>
					 </div>
					 </div>
				</div>
				<!-- top-grids -->

					<!-- /blog-pop-->
			<div class="modal ab fade" id="myModal6" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
				<div class="modal-dialog about" role="document">
					<div class="modal-content about">
						<div class="modal-header">
							<button type="button" class="close ab" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
						</div>
						<div class="modal-body about">
								<div class="about">

									  <div class="about-inner">

									      <img src="files/gecer_ganfit_workflow.png" alt="about" style="
    max-width: 715px;
">									     <h4 class="tittle">Abstract</h4>
										   <p>In the past few years, a lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of Deep Convolutional Neural Networks (DCNNs). In the most recent works, differentiable renderers were employed in order to learn the relationship between the facial identity features and the parameters of a 3D morphable model for shape and texture. The texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction of the state-of-the-art methods is still not capable of modeling textures in high fidelity. In this paper, we take a radically different approach and harness the power of Generative Adversarial Networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful generator of facial texture in UV space. Then, we revisit the original 3D Morphable Models (3DMMs) fitting approaches making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. We optimize the parameters with the supervision of pretrained deep identity features through our end-to-end differentiable framework. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.</p>
									  </div>

								</div>
						</div>
					</div>
				</div>
			</div>
			<!-- //blog-pop-->

      <!-- /blog-pop-->
      <div class="modal ab fade" id="myModal5" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
        <div class="modal-dialog about" role="document">
          <div class="modal-content about">
            <div class="modal-header">
              <button type="button" class="close ab" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
            </div>
            <div class="modal-body about">
                <div class="about">

                    <div class="about-inner">

                        <img src="files/gecer_facegan_workflow.png" alt="about" style="
    max-width: 715px;
    ">									     <h4 class="tittle">Abstract</h4>
                       <p>We propose a novel end-to-end semi-supervised adversarial framework to generate photorealistic face images of new identities with a wide range of expressions, poses, and illuminations conditioned by synthetic images sampled from a 3D morphable model. Previous adversarial style-transfer methods either supervise their networks with a large volume of paired data or train highly under-constrained two-way generative networks in an unsupervised fashion. We propose a semi-supervised adversarial learning framework to constrain the twoway networks by a small number of paired real and synthetic images, along with a large volume of unpaired data. A set-based loss is also proposed to preserve identity coherence of generated images. Qualitative results show that generated face images of new identities contain pose, lighting and expression diversity. They are also highly constrained by the synthetic input images while adding photorealism and retaining identity information. We combine face images generated by the proposed method with a real data set to train face recognition algorithms and evaluate the model quantitatively on two challenging data sets: LFW and IJB-A. The generated images by our framework consistently improve the performance of deep face recognition networks trained with the Oxford VGG Face dataset, and achieve comparable results to the state-of-the-art.</p>
                    </div>

                </div>
            </div>
          </div>
        </div>
      </div>
  <!-- //blog-pop-->

					<!-- /blog-pop-->
			<div class="modal ab fade" id="myModal4" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
				<div class="modal-dialog about" role="document">
					<div class="modal-content about">
						<div class="modal-header">
							<button type="button" class="close ab" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
						</div>
						<div class="modal-body about">
								<div class="about">

									  <div class="about-inner">

									      <img src="files/gecer_breast_workflow.png" alt="about" style="max-width: 715px;">
										  <h4 class="tittle">Highlights</h4>
										   <ul>
										   <li>Commonly studied scenario considers only binary cancer vs. no cancer classification.</li>
											<li>Our system classifies whole slide breast biopsies into five diagnostic categories.</li>
											<li>Pipeline of fully convolutional networks localizes diagnostically relevant regions.</li>
											<li>Convolutional neural network classifies detected regions of interest in whole slides.</li>
											<li>Experiments show that our method is compatible with predictions of 45 pathologists.</li>
										   </ul>
										  <h4 class="tittle">Abstract</h4>
										   <p>Generalizability of algorithms for binary cancer vs. no cancer classification is unknown for clinically more significant multi-class scenarios where intermediate categories have different risk factors and treatment strategies. We present a system that classifies whole slide images (WSI) of breast biopsies into five diagnostic categories. First, a saliency detector that uses a pipeline of four fully convolutional networks, trained with samples from records of pathologists’ screenings, performs multi-scale localization of diagnostically relevant regions of interest in WSI. Then, a convolutional network, trained from consensus-derived reference samples, classifies image patches as non-proliferative or proliferative changes, atypical ductal hyperplasia, ductal carcinoma in situ, and invasive carcinoma. Finally, the saliency and classification maps are fused for pixel-wise labeling and slide-level categorization. Experiments using 240 WSI showed that both saliency detector and classifier networks performed better than competing algorithms, and the five-class slide-level accuracy of 55% was not statistically different from the predictions of 45 pathologists. We also present example visualizations of the learned representations for breast cancer diagnosis.</p>
									  </div>

								</div>
						</div>
					</div>
				</div>
			</div>
			<!-- //blog-pop-->

					<!-- /blog-pop-->
			<div class="modal ab fade" id="myModal3" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
				<div class="modal-dialog about" role="document">
					<div class="modal-content about">
						<div class="modal-header">
							<button type="button" class="close ab" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
						</div>
						<div class="modal-body about">
								<div class="about">

									  <div class="about-inner">

									      <img src="files/gecer_maxmargin_workflow.png" alt="about" style="
    max-width: 715px;
">									     <h4 class="tittle">Abstract</h4>
										   <p>In this work, we investigate several methods and strategies to learn deep embeddings for face recognition, using joint sample- and set-based optimization. We explain our framework that expands traditional learning with set-based supervision together with the strategies used to maintain set characteristics. We, then, briefly review the related set-based loss functions, and subsequently we propose a novel Max-Margin Loss which maximizes maximum possible inter-class margin with assistance of Support Vector Machines (SVMs). It implicitly pushes all the samples towards correct side of the margin with a vector perpendicular to the hyperplane and a strength inversely proportional to the distance to it. We show that the introduced loss outperform the previous sample-based and set-based ones in terms verification of faces on two commonly used benchmarks.</p>
									  </div>

								</div>
						</div>
					</div>
				</div>
			</div>
			<!-- //blog-pop-->


	<!-- /blog-pop-->
			<div class="modal ab fade" id="myModal1" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
				<div class="modal-dialog about" role="document">
					<div class="modal-content about">
						<div class="modal-header">
							<button type="button" class="close ab" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
						</div>
						<div class="modal-body about">
								<div class="about">

									  <div class="about-inner">

									      <img src="files/gecer_IVC_2017_graphical_abstract.png" alt="about" style="max-width: 715px;">
										  <h4 class="tittle">Highlights</h4>
										   <ul>
										   <li>We propose novel color-blob-based COSFIRE filters.</li>
											<li>They are effective for recognizing also objects with diffuse region boundaries.</li>
											<li>Such a filter models (a part of) an object by a specific arrangement of color blobs.</li>
											<li>The blobs contain information about the sizes and colors of the interior of regions.</li>
											<li>We achieve high recognition rates: GTSRB (98.94%) and Butterfly (89.02%) data sets.</li>
										   </ul>
										     <h4 class="tittle">Abstract</h4>
										   <p>Most object recognition methods rely on contour-defined features obtained by edge detection or region segmentation. They are not robust to diffuse region boundaries. Furthermore, such methods do not exploit region color information. We propose color-blob-based COSFIRE (Combination of Shifted Filter Responses) filters to be selective for combinations of diffuse circular regions (blobs) in specific mutual spatial arrangements. Such a filter combines the responses of a certain selection of Difference-of-Gaussians filters, essentially blob detectors, of different scales, in certain channels of a color space, and at certain relative positions to each other. Its parameters are determined/learned in an automatic configuration process that analyzes the properties of a given prototype object of interest. We use these filters to compute features that are effective for the recognition of the prototype objects. We form feature vectors that we use with an SVM classifier. We evaluate the proposed method on a traffic sign (GTSRB) and a butterfly data sets. For the GTSRB data set we achieve a recognition rate of 98.94%, which is slightly higher than human performance and for the butterfly data set we achieve 89.02%. The proposed color-blob-based COSFIRE filters are very effective and outperform the contour-based COSFIRE filters. A COSFIRE filter is trainable, it can be configured with a single prototype pattern and it does not require domain knowledge.</p>
									  </div>

								</div>
						</div>
					</div>
				</div>
			</div>
			<!-- //blog-pop-->

	<!-- /blog-pop-->
			<div class="modal ab fade" id="myModal2" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true" style="display: none;">
				<div class="modal-dialog about" role="document">
					<div class="modal-content about">
						<div class="modal-header">
							<button type="button" class="close ab" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
						</div>
						<div class="modal-body about">
								<div class="about">

									  <div class="about-inner">

									      <img src="files/gecer_thesis_workflow.png" alt="about" style="
    max-width: 715px;
">
										     <h4 class="tittle">Abstract</h4>
										   <p>The most frequent non-skin cancer type is breast cancer which is also named one of the most deadliest diseases where early and accurate diagnosis is critical for recovery. Recent medical image processing researches have demonstrated promising results that may contribute to the analysis of biopsy images by enhancing the understanding or by revealing possible unhealthy tissues during diagnosis. However, these studies focused on well-annotated and -cropped patches, whereas a fully automated computer-aided diagnosis (CAD) system requires whole slide histopathology image (WSI) processing which is, in fact, enormous in size and, therefore, difficult to process with a reasonable computational power and time. Moreover, those whole slide biopsies consist of healthy, benign and cancerous tissues at various stages and thus, simultaneous detection and classiffication of diagnostically relevant regions are challenging.</p>
										   <p>We propose a complete CAD system for efficient localization and classification of regions of interest (ROI) in WSI by employing state-of-the-art deep learning techniques. The system is developed to resemble organized work ow of expert pathologists by means of progressive zooming into details, and it consists of two separate sequential steps: (1) detection of ROIs in WSI, (2) classification of the detected ROIs into five diagnostic classes. The novel saliency detection approach intends to mimic efficient search patterns of experts at multiple resolutions by training four separate deep networks with the samples extracted from the tracking records of pathologists' viewing of WSIs. The detected relevant regions are fed to the classification step that includes a deeper network that produces probability maps for classes, followed by a post-processing step for final diagnosis.</p>
										   <p>In the experiments with 240 WSI, the proposed saliency detection approach outperforms a state-of-the-art method by means of both efficiency and eectiveness, and the final classification of our complete system obtains slightly lower accuracy than the mean of 45 pathologists' performance. According to the Mc- Nemar's statistical tests, we cannot reject that the accuracies of 32 out of 45 pathologists are not different from the proposed system. At the end, we also provide visualizations of our deep model with several advanced techniques for better understanding of the learned features and the overall information captured by the network</p>
									  </div>

								</div>
						</div>
					</div>
				</div>
			</div>
			<!-- //blog-pop-->

			<!-- /header -->
<div class="footer" id="contact" style="min-height: 100%;">
	<div class="container">
	<div class="service-head one text-center">
						<h4>Last Update : 29/04/2020</h4>
						<h5 style="color: black;">© Baris Gecer. All right reserved</h5>
						<span class="border two"></span>
					</div>

		<div class="copy_right text-center">
			<p style="color: blue;">© 2016 Preface . All rights reserved | Design by <a href="http://w3layouts.com/" target="_blank">W3layouts.</a></p>
			<br>
			<p style="color: blue;">Footer Image Credit: Esra Nur Gecer</p>
		</div>
	</div>
</div>
			<!-- //footer -->
		<!-- /container -->

		<a href="#home" id="toTop" style="display: none;"><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover"></span><span id="toTopHover" style="opacity: 0;"></span><span id="toTopHover"></span> <span id="toTopHover" style="opacity: 1;"> </span></a>
	<!--start-smooth-scrolling-->
						<script type="text/javascript">
									$(document).ready(function() {
										/*
										var defaults = {
								  			containerID: 'toTop', // fading element id
											containerHoverID: 'toTopHover', // fading element hover id
											scrollSpeed: 1200,
											easingType: 'linear'
								 		};
										*/

										$().UItoTop({ easingType: 'easeOutQuart' });

									});
								</script>
								<!--end-smooth-scrolling-->
<!-- //for bootstrap working -->
	<script src="js/bootstrap.js"></script>





<a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a><a href="#" id="toTop">To Top</a></body></html>
